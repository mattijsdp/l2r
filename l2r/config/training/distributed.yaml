# Distributed Training Configuration

# Inherit from default training configuration
defaults:
  - default

# Distributed training settings
strategy: ddp
num_nodes: 4
gpus_per_node: 4
accelerator: gpu
precision: 16  # Use mixed precision

# Adjust batch size for distributed training
batch_size: 45  # For TSP (180 / 4 = 45) 
# For CVRP: 16 (64 / 4 = 16) 